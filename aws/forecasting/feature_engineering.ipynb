{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import datetime\n",
    "import math\n",
    "from itertools import chain\n",
    "import boto3\n",
    "import sagemaker as sm\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering, targets creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def add_spread(df):\n",
    "    df['spread'] = df.high - df.low\n",
    "    return df\n",
    "def add_weekly_cat(df):\n",
    "    day_cos, day_sin = list(), list()\n",
    "    for date in df.index:\n",
    "        day = datetime.datetime.strptime(date, '%Y-%m-%d').weekday()\n",
    "        radians = 2*math.pi*day/6\n",
    "        day_cos.append(math.cos(radians))\n",
    "        day_sin.append(math.sin(radians))\n",
    "    df['week_cos'] = day_cos\n",
    "    df['week_sin'] = day_sin\n",
    "    return df\n",
    "def add_yearly_cat(df):\n",
    "    day_cos, day_sin = list(), list()\n",
    "    for date in df.index:\n",
    "        day = datetime.datetime.strptime(date, '%Y-%m-%d').timetuple().tm_yday\n",
    "        radians = 2*math.pi*day/365\n",
    "        day_cos.append(math.cos(radians))\n",
    "        day_sin.append(math.sin(radians))\n",
    "    df['year_cos'] = day_cos\n",
    "    df['year_sin'] = day_sin\n",
    "    return df\n",
    "def add_percent_changes(df, metrics=None, periods=None, shifts=None): ## pure!!!\n",
    "    d = dict()\n",
    "    if metrics is None:\n",
    "        metrics=df.columns\n",
    "    for shift_len in shifts:\n",
    "        for metric in metrics:\n",
    "            for period in periods:\n",
    "                percents = df[metric].shift(shift_len).pct_change(periods=period)\n",
    "                d[f'{metric}_percent_change_percent_lag_{period}_shift_{shift_len}']=percents\n",
    "    df1 = pd.concat((df, pd.DataFrame(d)), axis=1)\n",
    "    return df1\n",
    "def add_stats(df, lengths=None, metrics=None, shifts=None, quantiles=None): ## pure!!!\n",
    "    d = dict()\n",
    "    if metrics is None:\n",
    "        metrics=df.columns\n",
    "    for shift_len in shifts:\n",
    "        for metric in metrics:\n",
    "            for length in lengths:\n",
    "                win = df[metric].shift(shift_len).rolling(length, min_periods=4)\n",
    "                low = win.min()\n",
    "                high = win.max()\n",
    "                d.update({\n",
    "                    f'{metric}_window_{length}_shift_{shift_len}_mean': win.mean(),\n",
    "                    f'{metric}_window_{length}_shift_{shift_len}_std' : win.std(),\n",
    "                    f'{metric}_window_{length}_shift_{shift_len}_skew' : win.skew(),\n",
    "                    f'{metric}_window_{length}_shift_{shift_len}_kurtosis' : win.kurt(),\n",
    "                    f'{metric}_window_{length}_shift_{shift_len}_high' : high,\n",
    "                    f'{metric}_window_{length}_shift_{shift_len}_low' : low,\n",
    "                    f'{metric}_window_{length}_shift_{shift_len}_spread' : high.subtract(low),\n",
    "                    **dict(zip([f'{metric}_window_{length}_shift_{shift_len}_quantile_{n:.2f}' for n in quantiles], [win.quantile(round(n,2)) for n in quantiles]))}) ## cycles through each quantile and zips a dictionary from it, then expands it\n",
    "    df1 = pd.concat((df, pd.DataFrame(d)), axis=1)\n",
    "    return df1\n",
    "def add_correlations(df): ## TODO\n",
    "    return df \n",
    "def add_percent_changes_between_windows(df): ## can be done by calling standard add_percent_changes on the metrics generated by stats. does not need own function\n",
    "    pass \n",
    "def add_features(df, \n",
    "                 standard_metrics=['open','close','high','spread','low','volume'], \n",
    "                 win_lens = [10,40,70,180, 360],\n",
    "                 period_offsets = [*range(1,7),*range(10,131,30)],\n",
    "                 shifts = [*range(0, 61, 15)],\n",
    "                 quantiles = [*np.arange(0.05,.25, 0.10), *np.arange(0.80,1.0, 0.05)]\n",
    "                ):\n",
    "    features_df = df\\\n",
    "    .pipe(add_spread)\\\n",
    "    .pipe(add_stats, lengths=win_lens, metrics=standard_metrics, shifts=shifts, quantiles=quantiles)\\\n",
    "    .pipe(add_percent_changes, metrics=standard_metrics, periods=period_offsets, shifts=shifts)\\\n",
    "    .pipe(add_weekly_cat)\\\n",
    "    .pipe(add_yearly_cat)\n",
    "    return features_df\n",
    "def add_percent_change_1_month_window_3_months_ahead(): ## TODO\n",
    "    return df\n",
    "def add_targets(df,\n",
    "                standard_metrics=['open','close','high','spread','low'], \n",
    "                win_lens = [*range(7,30,7)], \n",
    "                shifts = [*range(-180,-14, 14)],\n",
    "                quantiles = [*np.arange(0.05,.25, 0.10), *np.arange(0.80,1.0, 0.05)],\n",
    "                percent_metrics=['open','close','high','spread','low'],\n",
    "                percent_change_shifts = [*range(-180,-14, 14)], \n",
    "                period_offsets = [*range(1,10),*range(10,101,20)]\n",
    "                \n",
    "                ):\n",
    "    targets_df = df.pipe(add_stats, lengths=win_lens, metrics=standard_metrics, shifts=shifts, quantiles=quantiles)\\\n",
    "    .pipe(add_percent_changes, metrics=percent_metrics, periods=period_offsets, shifts=percent_change_shifts)\n",
    "    return targets_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 s ± 15.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard_metrics=['low']\n",
    "# win_lens = [20] \n",
    "# period_offsets = [90]\n",
    "# shifts = [-90]\n",
    "# percent_change_shifts = [0]\n",
    "# quantiles = [0.2]\n",
    "# percent_metrics=[f'{standard_metrics[0]}_window_{win_lens[0]}_shift_{shifts[0]}_mean']\n",
    "\n",
    "\n",
    "# df = pd.read_parquet('temp.parquet')\n",
    "# # df = add_spread(df)\n",
    "# features_df = df.pipe(add_features)\n",
    "# targets_df = df.pipe(add_targets, win_lens=win_lens, standard_metrics=standard_metrics, percent_metrics=percent_metrics, period_offsets=period_offsets, shifts=shifts, percent_change_shifts=percent_change_shifts, quantiles=quantiles)\n",
    "# df = pd.concat([targets_df, features_df], keys=['targets', 'features'], axis=1)\n",
    "# target_col = targets_df['low_window_20_shift_-90_mean_percent_change_percent_lag_90_shift_0']\n",
    "# df_algo = pd.concat([target_col, features_df], axis=1)\n",
    "# display('df shape', df.shape, 'no null shapes', df.dropna(axis=1).shape, df.dropna(axis=0).shape)\n",
    "# display('targets info', df.targets.info(), 'features info', df.features.info())\n",
    "# display('nulls total', df.isna().sum().describe())\n",
    "# display('total df info', df.info(), df.describe(), df)\n",
    "# display(df_algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# file manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ##constants\n",
    "    standard_metrics=['low']\n",
    "    win_lens = [20] \n",
    "    period_offsets = [90]\n",
    "    shifts = [-90]\n",
    "    percent_change_shifts = [0]\n",
    "    quantiles = [0.2]\n",
    "    percent_metrics=[f'{standard_metrics[0]}_window_{win_lens[0]}_shift_{shifts[0]}_mean']\n",
    "    ##code\n",
    "    df = pd.read_parquet('temp.parquet')\n",
    "    features_df = df.pipe(add_features)\n",
    "    targets_df = df.pipe(add_targets, win_lens=win_lens, standard_metrics=standard_metrics, percent_metrics=percent_metrics, period_offsets=period_offsets, shifts=shifts, percent_change_shifts=percent_change_shifts, quantiles=quantiles)\n",
    "    target_col = targets_df['low_window_20_shift_-90_mean_percent_change_percent_lag_90_shift_0']\n",
    "    df_algo = pd.concat([target_col, features_df], axis=1)\n",
    "    df_algo.dropna(axis=0, inplace=True)\n",
    "    df_algo.to_csv('xgboost_demo.csv', header=False)\n",
    "\n",
    "def transform_data_to_s3():\n",
    "    bucket = sm.Session().default_bucket()\n",
    "    subfolder='stock-data-raw'\n",
    "    s3c = boto3.client('s3')\n",
    "    s3f = s3fs.S3FileSystem()\n",
    "    f_names = s3f.ls(f'{bucket}/{subfolder}')\n",
    "    new_folder = 'stock_data_raw_with_3_mo_target'\n",
    "    for i, file_name in enumerate(f_names):\n",
    "        df = pq.ParquetDataset(f's3://{file_name}', filesystem=s3f).read_pandas().to_pandas()\n",
    "        target_added_df = main_target_add(df)\n",
    "        target_added_df.to_csv('temp.csv')\n",
    "        new_file_name = file_name.split('/')[-1].replace('.parquet', '_with_3_mo_target.csv')\n",
    "        new_file_path = '/'.join([new_folder, new_file_name])\n",
    "        s3c.upload_file('temp.csv', bucket, new_file_path)\n",
    "        if i%100==0:\n",
    "            display(i)\n",
    "def main_target_add(df):\n",
    "    ##constants\n",
    "    standard_metrics=['low']\n",
    "    win_lens = [20] \n",
    "    period_offsets = [90]\n",
    "    shifts = [0]\n",
    "    percent_change_shifts = [-90]\n",
    "    quantiles = [0.2]\n",
    "    percent_metrics=[f'{standard_metrics[0]}_window_{win_lens[0]}_shift_{shifts[0]}_mean']\n",
    "    ##code\n",
    "    targets_df = df.pipe(add_targets, win_lens=win_lens, standard_metrics=standard_metrics, percent_metrics=percent_metrics, period_offsets=period_offsets, shifts=shifts, percent_change_shifts=percent_change_shifts, quantiles=quantiles)\n",
    "    target_col = targets_df[f'{percent_metrics[0]}_percent_change_percent_lag_{period_offsets[0]}_shift_{percent_change_shifts[0]}']\n",
    "    df_algo = pd.concat([target_col, df], axis=1)\n",
    "    return df_algo\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1300"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1400"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1700"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1900"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2200"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2300"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2600"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2700"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2800"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# main()\n",
    "transform_data_to_s3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-01-29</th>\n",
       "      <td>9.50</td>\n",
       "      <td>9.9900</td>\n",
       "      <td>8.57</td>\n",
       "      <td>8.75</td>\n",
       "      <td>1489121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-30</th>\n",
       "      <td>8.75</td>\n",
       "      <td>9.1500</td>\n",
       "      <td>8.30</td>\n",
       "      <td>8.50</td>\n",
       "      <td>218958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-01-31</th>\n",
       "      <td>8.49</td>\n",
       "      <td>10.3000</td>\n",
       "      <td>8.49</td>\n",
       "      <td>9.55</td>\n",
       "      <td>182234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-01</th>\n",
       "      <td>9.93</td>\n",
       "      <td>9.9400</td>\n",
       "      <td>9.50</td>\n",
       "      <td>9.51</td>\n",
       "      <td>28109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-04</th>\n",
       "      <td>9.50</td>\n",
       "      <td>9.7120</td>\n",
       "      <td>9.50</td>\n",
       "      <td>9.50</td>\n",
       "      <td>8234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-25</th>\n",
       "      <td>4.00</td>\n",
       "      <td>4.1399</td>\n",
       "      <td>3.77</td>\n",
       "      <td>3.97</td>\n",
       "      <td>207284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-26</th>\n",
       "      <td>4.30</td>\n",
       "      <td>5.4400</td>\n",
       "      <td>4.30</td>\n",
       "      <td>4.92</td>\n",
       "      <td>5809520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-29</th>\n",
       "      <td>4.39</td>\n",
       "      <td>4.6800</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.36</td>\n",
       "      <td>714390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-30</th>\n",
       "      <td>4.42</td>\n",
       "      <td>4.4500</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.27</td>\n",
       "      <td>265390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-31</th>\n",
       "      <td>4.20</td>\n",
       "      <td>4.7500</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.37</td>\n",
       "      <td>2152571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3317 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            open     high   low  close   volume\n",
       "2008-01-29  9.50   9.9900  8.57   8.75  1489121\n",
       "2008-01-30  8.75   9.1500  8.30   8.50   218958\n",
       "2008-01-31  8.49  10.3000  8.49   9.55   182234\n",
       "2008-02-01  9.93   9.9400  9.50   9.51    28109\n",
       "2008-02-04  9.50   9.7120  9.50   9.50     8234\n",
       "...          ...      ...   ...    ...      ...\n",
       "2021-03-25  4.00   4.1399  3.77   3.97   207284\n",
       "2021-03-26  4.30   5.4400  4.30   4.92  5809520\n",
       "2021-03-29  4.39   4.6800  4.20   4.36   714390\n",
       "2021-03-30  4.42   4.4500  4.20   4.27   265390\n",
       "2021-03-31  4.20   4.7500  4.20   4.37  2152571\n",
       "\n",
       "[3317 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bucket = sm.Session().default_bucket()\n",
    "subfolder='stock-data-raw'\n",
    "\n",
    "# s3c = boto3.client('s3')\n",
    "# contents = s3c.list_objects(Bucket=bucket, Prefix=subfolder)['Contents']\n",
    "# #     for content in contents:\n",
    "# display(contents[0]['Key'].replace('.parquet', '_with_3_mo_target.csv'))\n",
    "\n",
    "\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "s3f = s3fs.S3FileSystem()\n",
    "\n",
    "df = pq.ParquetDataset(f's3://{bucket}/{subfolder}/AACG_daily_raw.parquet', filesystem=s3).read_pandas().to_pandas()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df = df.pipe(add_)\n",
    "# display(len(df.columns))\n",
    "# standard_metrics=['open']#,'close','high','low','volume']\n",
    "# win_lens = [*range(10,180,10),*range(182,365,91)]\n",
    "# period_offsets = [*range(1,10),*range(10,101,10)]\n",
    "# display('40+ times this many metrics', len(standard_metrics)*len(win_lens)**2*len(period_offsets)) \n",
    "# %timeit df.pipe(add_stats, lengths=win_lens, metrics=standard_metrics, shifts=win_lens)\n",
    "# for these metrics: 5 times: 20.8 s ± 213 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "# <class 'pandas.core.frame.DataFrame'>\n",
    "# Index: 4578 entries, 2002-12-31 to 2021-04-08\n",
    "# Columns: 16650 entries, open to open_length_364_shift_364_spread\n",
    "# dtypes: float64(16649), int64(1)\n",
    "# memory usage: 581.6+ MB\n",
    "## would be a total of 2.5+ GB per stock or ~12.5+ terabytes for all stocks in NYSE\n",
    "# df = df.pipe(add_stats, lengths=win_lens, metrics=standard_metrics, shifts=win_lens)\n",
    "# display(df.info(memory_usage='deep'))\n",
    "\n",
    "# standard_metrics=['open','close','high','low','volume']\n",
    "# win_lens = [*range(7,30,7)]\n",
    "# period_offsets = [*range(1,10),*range(10,101,10)]\n",
    "# shifts = [*range(-180,-14, 14)]\n",
    "# display('40+ times this many metrics', len(standard_metrics)*len(win_lens)*len(period_offsets)*len(shifts)) \n",
    "\n",
    "## for these metrics: 10.7 s ± 117 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "# %timeit  add_targets(df, standard_metrics=standard_metrics)\n",
    "# df = df.pipe(add_targets, standard_metrics=['open'])\n",
    "# display(df.info(memory_usage='deep'))\n",
    "# <class 'pandas.core.frame.DataFrame'>\n",
    "# Index: 4578 entries, 2002-12-31 to 2021-04-08\n",
    "# Columns: 16650 entries, open to open_length_364_shift_364_spread\n",
    "# dtypes: float64(16649), int64(1)\n",
    "# memory usage: 581.8 MB\n",
    "\n",
    "# standard_metrics=['open','close','high','spread','low','volume']\n",
    "# win_lens = [*range(10,180,30),*range(182,365,91)]\n",
    "# period_offsets = [*range(1,10),*range(10,101,10)]\n",
    "# shifts = [*range(0,180, 14)]\n",
    "# display('40+ times this many metrics', len(standard_metrics)*len(win_lens)*len(shifts)*len(period_offsets)) \n",
    "# df = df.pipe(add_features)\n",
    "# display(df.info(memory_usage='deep'))\n",
    "# %timeit df.pipe(add_features)\n",
    "# <class 'pandas.core.frame.DataFrame'>\n",
    "# Index: 4578 entries, 2002-12-31 to 2021-04-08\n",
    "# Columns: 9142 entries, open to year_sin\n",
    "# dtypes: float64(9141), int64(1)\n",
    "# memory usage: 319.6 MB\n",
    "## 14.3 s ± 123 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    "# standard_metrics=['open','close','high','spread','low','volume'], \n",
    "#                  win_lens = [10,40,70,180,360],\n",
    "#                  period_offsets = [*range(1,7),*range(10,91,20)],\n",
    "#                  shifts = [*range(0, 180, 14)],\n",
    "#                  quantiles = [*np.arange(0.05,.25, 0.10), *np.arange(0.80,1.0, 0.05)]\n",
    "# df = df.pipe(add_features)\n",
    "# display(df.info())\n",
    "# %timeit df.pipe(add_features)\n",
    "# <class 'pandas.core.frame.DataFrame'>\n",
    "# Index: 4578 entries, 2002-12-31 to 2021-04-08\n",
    "# Columns: 5086 entries, open to year_sin\n",
    "# dtypes: float64(5085), int64(1)\n",
    "# memory usage: 177.7+ MB\n",
    "# 7.s74 s ± 96.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "# df = df.pipe(add_targets)\n",
    "# display(df.info())\n",
    "# %timeit df.pipe(add_targets)\n",
    "# <class 'pandas.core.frame.DataFrame'>\n",
    "# Index: 4578 entries, 2002-12-31 to 2021-04-08\n",
    "# Columns: 3131 entries, open to low_percent_change_90_lag\n",
    "# dtypes: float64(3130), int64(1)\n",
    "# memory usage: 109.4+ MB\n",
    "# 4.25 s ± 14.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "## with individual df['']=... in add_stats:\n",
    "# 506 ms ± 37.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "# %timeit df.pipe(add_stats, lengths=[10,40,70,180,360],\\\n",
    "#                 metrics=['open','close','high','low','volume'],\\\n",
    "#                 shifts=[*range(0, 60, 14)],\\\n",
    "#                 quantiles=[*np.arange(0.05,.25, 0.10), *np.arange(0.80,1.0, 0.05)])\n",
    "## with creating a list of dicts and concatenating them:\n",
    "## notes for speed tests\n",
    "#                 dicts.append({\n",
    "#                     f'{metric}_length_{length}_shift_{shift_len}_mean' : win.mean(),\n",
    "#                     f'{metric}_length_{length}_shift_{shift_len}_std' : win.std(),\n",
    "#                     f'{metric}_length_{length}_shift_{shift_len}_skew' : win.skew(),\n",
    "#                     f'{metric}_length_{length}_shift_{shift_len}_kurtosis' : win.kurt(),\n",
    "#                     f'{metric}_length_{length}_shift_{shift_len}_high' : win.max(),\n",
    "#                     f'{metric}_length_{length}_shift_{shift_len}_low' : win.min()\n",
    "#                 })\n",
    "#     df = pd.concat([df, *[pd.DataFrame(d for d in dicts)]])\n",
    "# 806 ms ± 18.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "# %timeit df.pipe(add_features)\n",
    "# df = df.pipe(add_features)\n",
    "# display(df.info(memory_usage='deep'))\n",
    "# display(df)\n",
    "# latest without shift changes in the percent changes:\n",
    "# 3.18 s ± 105 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "# <class 'pandas.core.frame.DataFrame'>\n",
    "# Index: 4578 entries, 2002-12-31 to 2021-04-08\n",
    "# Columns: 1966 entries, open to year_sin\n",
    "# dtypes: float64(1965), int64(1)\n",
    "# memory usage: 69.0 MB\n",
    "## with shift changes in percent changes. I believe the concatenating is a large part of the time to take. \n",
    "# 3.14 s ± 112 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "# <class 'pandas.core.frame.DataFrame'>\n",
    "# Index: 4578 entries, 2002-12-31 to 2021-04-08\n",
    "# Columns: 1990 entries, open to year_sin\n",
    "# dtypes: float64(1989), int64(1)\n",
    "# memory usage: 69.8 MB\n",
    "## times 5000 stocks: 350 gigabytes; 4 hours. \n",
    "## needs to speed up in some way to add in the next 14-100 features with ~1000x feature engineering \n",
    "## needs memory minimization for the 100x+ additional feature engineering; what are ways to break up into smaller memory? can the preprocessing be done with batches for the training algorithm; time-memory trade-off. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to try formatting, reading training with small subsets of data, small feature set, and test efficacy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras for keras manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def to_ts_df(daily_stocks_data, lookback, metric):\n",
    "#     ## column names\n",
    "#     columns = list()\n",
    "#     for i in range(lookback):\n",
    "#         columns.append(f'{metric}_{i}')\n",
    "#     columns.append(f'{metric}_target')\n",
    "#     df = pd.DataFrame(columns=columns)\n",
    "#     ## columns\n",
    "#     data = daily_stocks_data[metric].to_numpy()\n",
    "#     for index, col in enumerate(df.columns):\n",
    "#         df[col] = data[index:len(data)-lookback+index]\n",
    "#     ## dates index\n",
    "#     dates = daily_stocks_data.date.to_numpy()[:-lookback]\n",
    "#     df.insert(0, 'dates', dates)\n",
    "#     df.dropna(axis='index', inplace=True)\n",
    "#     return df\n",
    "# def to_ts(df, metric, lookback):\n",
    "#     data, targets = list(), list()\n",
    "#     for i in range(lookback,len(df.index)):\n",
    "#         data.append(df.iloc[i-lookback:i,:].values) ## first four metrics\n",
    "#         targets.append(df[metric].to_list()[i])\n",
    "#     data = np.array(data)\n",
    "#     targets = np.array(targets)\n",
    "#     return data, targets\n",
    "# def min_max_scale(col):\n",
    "#     scaled = col.subtract(col.min()).divide(col.max()-col.min())\n",
    "#     return scaled\n",
    "# def multi_stock_ts_split(df,tickers): ## could be sped up \n",
    "#     data_tr, data_te, targets_tr, targets_te = [],[],[],[]\n",
    "#     for ticker in tickers:\n",
    "#         data, targets = to_ts(df[ticker].dropna(), 'low', lookback) ## drops nan for each stock\n",
    "#         x = train_test_split(data, targets, shuffle=False)\n",
    "#         data_tr.append(x[0])\n",
    "#         data_te.append(x[1])\n",
    "#         targets_tr.append(x[2]) \n",
    "#         targets_te.append(x[3])    \n",
    "#     return np.concatenate(data_tr), np.concatenate(data_te), np.concatenate(targets_tr), np.concatenate(targets_te)\n",
    "# df = pd.read_pickle(f\"./{tickers}_daily.pkl\")\n",
    "# df = df.apply(min_max_scale)\n",
    "# features = len(df.columns)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
