{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "silver-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stock_data_ingestion import ingest_stocks_to_df\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display \n",
    "from keras.layers import LSTM, Dense, Input, Dropout, Activation, BatchNormalization, LayerNormalization, GRU, Bidirectional\n",
    "from keras import Sequential\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.losses import MeanSquaredError\n",
    "import numpy as np\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-thompson",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "contained-bronze",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_targets(stock_arr, sequence_inlength):\n",
    "    \"\"\"where stock_arr is sequential np.array and \n",
    "    sequence_inlength is the length of the data sequences\"\"\"\n",
    "    x, y = list(), list()\n",
    "    for index in range(sequence_inlength, len(stock_arr)):\n",
    "        window = stock_arr[index-sequence_inlength:index]\n",
    "        x.append(window)\n",
    "        y.append([stock_arr[index]])\n",
    "    data = np.array(x)\n",
    "    targets = np.array(y)\n",
    "    return data, targets\n",
    "def test_train_split(data, targets, split=0.8):\n",
    "    \"\"\"both of np.array\"\"\"\n",
    "    data_train_len = int(len(data)*split)\n",
    "    targets_train_len = int(len(targets)*split)\n",
    "    data_tr, targets_tr = data[:data_train_len], targets[:targets_train_len]\n",
    "    data_test, targets_test = data[data_train_len:], targets[targets_train_len:]\n",
    "    return data_tr, targets_tr, data_test, targets_test\n",
    "def to_ts_data(stock=stock_list[0], sequence_inlength=60):\n",
    "    stock_arr = stocks_df[stocks_df.company_name == stock]['Adj Close'].to_numpy()\n",
    "    data, targets = get_data_targets(stock_arr, sequence_inlength)\n",
    "#     display(data, targets)\n",
    "    data_tr, targets_tr, data_test, targets_test = test_train_split(data, targets, split=0.8)\n",
    "    data_gen_train = TimeseriesGenerator(data_tr, targets_tr, length=sequence_inlength)\n",
    "    data_gen_test = TimeseriesGenerator(data_test, targets_test, length=sequence_inlength)\n",
    "    ##shape (number of batches, input number of steps before prediction, feature-dimensions for input)\n",
    "    return data_gen_train, data_gen_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "uniform-precipitation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## comment first line out after first use to prevent \n",
    "## giving the source website for the stocks too much traffic. \n",
    "# stocks_df = ingest_stocks_to_df()\n",
    "# stock_list = stocks_df.company_name.unique()\n",
    "# # displays\n",
    "# display(stocks_df)\n",
    "# for stock in stock_list:\n",
    "#     print(stock, 'len', len(stocks_df[stocks_df.company_name == stock].index))\n",
    "# sns.lineplot(x=stocks_df.index, y='Adj Close',data=stocks_df, legend='auto', hue='company_name')\n",
    "# plt.show()\n",
    "# data_gen_train, data_gen_test = to_ts_data() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-sustainability",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "nutritional-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(nodes_lstm=50, dropout=0.0, recurrent_dropout=0.0, learning_rate=0.01, loss='mse', optimizer=Adam, metrics=['mse']):\n",
    "    lstm_layer = LSTM(nodes_lstm, recurrent_dropout=recurrent_dropout)\n",
    "    model = Sequential()\n",
    "#     model.add(Input((1,1)))\n",
    "    model.add(lstm_layer)\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=metrics)\n",
    "    return model\n",
    "def seq_lstm_model(nodes_lstm=50, dropout=0.0, recurrent_dropout=0.0, learning_rate=0.01, loss='mse', optimizer=Adam, metrics=['mse']):\n",
    "    lstm_layer = LSTM(nodes_lstm, recurrent_dropout=recurrent_dropout, return_sequences=True)\n",
    "    model = Sequential()\n",
    "    model.add(lstm_layer)\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=metrics)\n",
    "    return model\n",
    "def bi_seq_lstm_model(nodes_lstm=50, dropout=0.0, recurrent_dropout=0.0, learning_rate=0.01, loss='mse', optimizer=Adam, metrics=['mse']):\n",
    "    lstm_layer = LSTM(nodes_lstm, recurrent_dropout=recurrent_dropout, return_sequences=True)\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(lstm_layer))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=metrics)\n",
    "    return model\n",
    "def stack_lstm_model(nodes_lstm=50, dropout=0.0, recurrent_dropout=0.0, learning_rate=0.01, loss='mse', optimizer=Adam, metrics=['mse']):\n",
    "    lstm_layer = LSTM(nodes_lstm, dropout=dropout, recurrent_dropout=recurrent_dropout, return_sequences=True)\n",
    "    model = Sequential()\n",
    "    model.add(lstm_layer)\n",
    "    model.add(lstm_layer)\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=metrics)\n",
    "    return model\n",
    "def bi_stack_lstm_model(nodes_lstm=50, dropout=0.0, recurrent_dropout=0.0, learning_rate=0.01, loss='mse', optimizer=Adam, metrics=['mse']):\n",
    "    lstm_layer = LSTM(nodes_lstm, recurrent_dropout=recurrent_dropout, return_sequences=True)\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(lstm_layer))\n",
    "    model.add(Bidirectional(lstm_layer))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=metrics)\n",
    "    return model\n",
    "def bi_back_lstm_model(nodes_lstm=50, dropout=0.0, recurrent_dropout=0.0, learning_rate=0.01, loss='mse', optimizer=Adam, metrics=['mse']):\n",
    "    lstm_layer = LSTM(nodes_lstm, recurrent_dropout=recurrent_dropout, return_sequences=True)\n",
    "    model = Sequential()\n",
    "    forward_layer = LSTM(nodes_lstm, return_sequences=True)\n",
    "    backward_layer = LSTM(nodes_lstm, return_sequences=True,\n",
    "                       go_backwards=True)\n",
    "    model.add(Bidirectional(forward_layer, backward_layer=backward_layer))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=metrics)\n",
    "    return model\n",
    "def bi_back_stack_lstm_model(nodes_lstm=50, dropout=0.0, recurrent_dropout=0.0, learning_rate=0.01, loss='mse', optimizer=Adam, metrics=['mse']):\n",
    "    model = Sequential()\n",
    "    forward_layer = LSTM(nodes_lstm, return_sequences=True)\n",
    "    backward_layer = LSTM(nodes_lstm, return_sequences=True, go_backwards=True)\n",
    "    model.add(Bidirectional(forward_layer, backward_layer=backward_layer))\n",
    "    model.add(Bidirectional(forward_layer, backward_layer=backward_layer))\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=metrics)\n",
    "    return model\n",
    "def stack_3_lstm_model(nodes_lstm=50, dropout=0.0, recurrent_dropout=0.0, learning_rate=0.01, loss='mse', optimizer=Adam, metrics=['mse']):\n",
    "    lstm_layer = LSTM(nodes_lstm, dropout=dropout, recurrent_dropout=recurrent_dropout, return_sequences=True)\n",
    "    model = Sequential()\n",
    "    model.add(lstm_layer)\n",
    "    model.add(lstm_layer)\n",
    "    model.add(lstm_layer)\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=metrics)\n",
    "    return model\n",
    "def deep_lstm_model(nodes_lstm=50, dropout=0.0, recurrent_dropout=0.0, learning_rate=0.01, loss='mse', optimizer=Adam, metrics=['mse']):\n",
    "    lstm_layer = LSTM(nodes_lstm, recurrent_dropout=recurrent_dropout)\n",
    "    model = Sequential()\n",
    "#     model.add(Input((1,1)))\n",
    "    model.add(lstm_layer)\n",
    "    model.add(Dense(32))    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation(relu))\n",
    "    model.add(Dense(32))    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation(relu))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=metrics)\n",
    "    return model\n",
    "def deep_lstm_model(nodes_lstm=50, dropout=0.0, recurrent_dropout=0.0, learning_rate=0.01, loss='mse', optimizer=Adam, metrics=['mse']):\n",
    "    lstm_layer = LSTM(nodes_lstm, recurrent_dropout=recurrent_dropout)\n",
    "    model = Sequential()\n",
    "#     model.add(Input((1,1)))\n",
    "    model.add(lstm_layer)\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(32))    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation(relu))\n",
    "    model.add(Dense(32))    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation(relu))\n",
    "    model.add(Dense(32))    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation(relu))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=metrics)\n",
    "    return model\n",
    "def lstm_dense_model(nodes_lstm=50, dropout=0.0, recurrent_dropout=0.0, learning_rate=0.01, loss='mse', optimizer=Adam, metrics=['mse']):\n",
    "    lstm_layer = LSTM(nodes_lstm, dropout=dropout, recurrent_dropout=recurrent_dropout)\n",
    "    model = Sequential()\n",
    "#     model.add(Input((1,1)))\n",
    "    model.add(lstm_layer)\n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation(relu))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=metrics)\n",
    "    return model\n",
    "def batch_norm_lstm_model(nodes_lstm=50, dropout=0.5, recurrent_dropout=0.0, learning_rate=0.01, loss='mse', optimizer=Adam, metrics=['mse']):\n",
    "    lstm_layer = LSTM(nodes_lstm, recurrent_dropout=recurrent_dropout)\n",
    "    model = Sequential()\n",
    "#     model.add(Input((1,1)))\n",
    "    model.add(lstm_layer)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(32))    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(relu))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=metrics)\n",
    "    return model\n",
    "def norm_lstm_model(nodes_lstm=50, dropout=0.5, recurrent_dropout=0.0, learning_rate=0.01, loss='mse', optimizer=Adam, metrics=['mse']):\n",
    "    lstm_layer = LSTM(nodes_lstm, recurrent_dropout=recurrent_dropout)\n",
    "    model = Sequential()\n",
    "#     model.add(Input((1,1)))\n",
    "    model.add(lstm_layer)\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(Dense(32))    \n",
    "    model.add(LayerNormalization())\n",
    "    model.add(Activation(relu))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=metrics)\n",
    "    return model\n",
    "# lstm_model().summary()\n",
    "def batch_norm_drop_lstm_model(nodes_lstm=50, dropout=0.5, recurrent_dropout=0.0, learning_rate=0.01, loss='mse', optimizer=Adam, metrics=['mse']):\n",
    "    lstm_layer = LSTM(nodes_lstm, recurrent_dropout=recurrent_dropout)\n",
    "    model = Sequential()\n",
    "#     model.add(Input((1,1)))\n",
    "    model.add(lstm_layer)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(32))    \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Activation(relu))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=metrics)\n",
    "    return model\n",
    "def gru_model(nodes=50, dropout=0.0, recurrent_dropout=0.0, learning_rate=0.01, loss='mse', optimizer=Adam, metrics=['mse']):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(nodes))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-archives",
   "metadata": {},
   "source": [
    "## model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "pharmaceutical-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "indirect-nutrition",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 4s 62ms/step - loss: 21.4959 - mse: 21.4959\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 0s 55ms/step - loss: 2.7365 - mse: 2.7365\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 2.7409 - mse: 2.7409\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 4.5086 - mse: 4.5086\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 0s 52ms/step - loss: 1.6663 - mse: 1.6663\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 1.9223 - mse: 1.9223\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 2.6338 - mse: 2.6338\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 2.2477 - mse: 2.2477\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 87ms/step - loss: 2.6739 - mse: 2.6739\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 2.4759 - mse: 2.4759\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 98ms/step - loss: 2.1963 - mse: 2.1963\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 2.1759 - mse: 2.1759\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 2.6579 - mse: 2.6579\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 2.3477 - mse: 2.3477\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 87ms/step - loss: 3.4264 - mse: 3.4264\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 3.8297 - mse: 3.8297\n",
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_50 (LSTM)               (None, 50)                22200     \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 22,251\n",
      "Trainable params: 22,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb69e3a5790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 33ms/step - loss: 0.5734 - mse: 0.5734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.57341068983078, 0.57341068983078]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lstm_model()\n",
    "model.fit(data_gen_train, epochs=100, callbacks=[early_stop])\n",
    "model.summary()\n",
    "model.evaluate(data_gen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "casual-setup",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 2s 41ms/step - loss: 19.1531 - mse: 19.1531\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 3.7305 - mse: 3.7305\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.0354 - mse: 2.0354\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 3.2770 - mse: 3.2770\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 3.1778 - mse: 3.1778\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.2695 - mse: 2.2695\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 3.4121 - mse: 3.4121\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 0s 40ms/step - loss: 3.0558 - mse: 3.0558\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.1883 - mse: 2.1883\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.9685 - mse: 2.9685\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 2.7096 - mse: 2.7096\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 0s 50ms/step - loss: 2.8496 - mse: 2.8496\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 0s 49ms/step - loss: 2.8167 - mse: 2.8167\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 1.9730 - mse: 1.9730\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 0s 43ms/step - loss: 3.3804 - mse: 3.3804\n",
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_51 (LSTM)               (None, None, 50)          22200     \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, None, 1)           51        \n",
      "=================================================================\n",
      "Total params: 22,251\n",
      "Trainable params: 22,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb6ae9c4280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 22ms/step - loss: 0.6208 - mse: 0.6208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6208373308181763, 0.6208373308181763]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = seq_lstm_model()\n",
    "model.fit(data_gen_train, epochs=100, callbacks=[early_stop])\n",
    "model.summary()\n",
    "model.evaluate(data_gen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fleet-moisture",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 5s 84ms/step - loss: 6.1043 - mse: 6.1043\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 3.8328 - mse: 3.8328\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 2.5214 - mse: 2.5214\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 80ms/step - loss: 2.9117 - mse: 2.9117\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 3.0211 - mse: 3.0211\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 2.8933 - mse: 2.8933\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 3.6974 - mse: 3.6974\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 1.8647 - mse: 1.8647\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 2.6132 - mse: 2.6132\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 107ms/step - loss: 2.7378 - mse: 2.7378\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 165ms/step - loss: 2.3363 - mse: 2.3363\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 106ms/step - loss: 1.9418 - mse: 1.9418\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 1s 142ms/step - loss: 2.6044 - mse: 2.6044\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 146ms/step - loss: 2.2393 - mse: 2.2393\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 134ms/step - loss: 2.6263 - mse: 2.6263\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 1s 100ms/step - loss: 2.3402 - mse: 2.3402\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 1s 114ms/step - loss: 2.5552 - mse: 2.5552\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 1s 103ms/step - loss: 2.0789 - mse: 2.0789\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 3.5840 - mse: 3.5840\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 1s 113ms/step - loss: 2.5594 - mse: 2.5594\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 2.2816 - mse: 2.2816\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 1s 91ms/step - loss: 2.3059 - mse: 2.3059\n",
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, None, 100)         44400     \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, None, 1)           101       \n",
      "=================================================================\n",
      "Total params: 44,501\n",
      "Trainable params: 44,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb6abe3f550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 40ms/step - loss: 0.8557 - mse: 0.8557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8556870222091675, 0.8556870222091675]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## bi is worse than normal performance\n",
    "model = bi_seq_lstm_model()\n",
    "model.fit(data_gen_train, epochs=100, callbacks=[early_stop])\n",
    "model.summary()\n",
    "model.evaluate(data_gen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "accomplished-interpretation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 3s 54ms/step - loss: 17.6332 - mse: 17.6332\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 2.6510 - mse: 2.6510\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 5.0003 - mse: 5.0003\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 2.5484 - mse: 2.5484\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 0s 51ms/step - loss: 2.2173 - mse: 2.2173\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 0s 51ms/step - loss: 3.5303 - mse: 3.5303\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 2.3582 - mse: 2.3582\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 0s 56ms/step - loss: 3.0422 - mse: 3.0422\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 3.0257 - mse: 3.0257\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 0s 51ms/step - loss: 2.0737 - mse: 2.0737\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 0s 55ms/step - loss: 2.4150 - mse: 2.4150\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 3.7252 - mse: 3.7252\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 0s 54ms/step - loss: 2.8649 - mse: 2.8649\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 54ms/step - loss: 3.8027 - mse: 3.8027\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 4.1725 - mse: 4.1725\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 2.9814 - mse: 2.9814\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 2.8079 - mse: 2.8079\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 0s 49ms/step - loss: 3.0485 - mse: 3.0485\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 0s 49ms/step - loss: 2.9588 - mse: 2.9588\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 0s 52ms/step - loss: 2.7003 - mse: 2.7003\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 3.1003 - mse: 3.1003\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 1s 55ms/step - loss: 1.8020 - mse: 1.8020\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 2.5965 - mse: 2.5965\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 3.3842 - mse: 3.3842\n",
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_52 (LSTM)               (None, None, 50)          22200     \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, None, 1)           51        \n",
      "=================================================================\n",
      "Total params: 22,251\n",
      "Trainable params: 22,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb6a15b9e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 20ms/step - loss: 0.5036 - mse: 0.5036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5035983920097351, 0.5035983920097351]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = stack_lstm_model()\n",
    "model.fit(data_gen_train, epochs=100, callbacks=[early_stop])\n",
    "model.summary()\n",
    "model.evaluate(data_gen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "august-armstrong",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 14s 258ms/step - loss: 8.5756 - mse: 8.5756\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 2s 214ms/step - loss: 2.4571 - mse: 2.4571\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 2s 174ms/step - loss: 3.3495 - mse: 3.3495\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 2s 276ms/step - loss: 2.1248 - mse: 2.1248\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 2s 206ms/step - loss: 2.4043 - mse: 2.4043\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 2s 227ms/step - loss: 2.7888 - mse: 2.7888\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 2.7376 - mse: 2.7376\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 2s 181ms/step - loss: 3.1392 - mse: 3.1392\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 2s 227ms/step - loss: 1.7792 - mse: 1.7792\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 2s 274ms/step - loss: 3.3027 - mse: 3.3027\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 2s 208ms/step - loss: 2.3645 - mse: 2.3645\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 2s 177ms/step - loss: 2.6762 - mse: 2.6762\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 2s 206ms/step - loss: 2.2842 - mse: 2.2842\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 2s 181ms/step - loss: 2.3532 - mse: 2.3532\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 2s 174ms/step - loss: 3.3011 - mse: 3.3011\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 2s 177ms/step - loss: 2.9225 - mse: 2.9225\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 3.1752 - mse: 3.1752\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 2s 180ms/step - loss: 3.7415 - mse: 3.7415\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 2s 215ms/step - loss: 2.9902 - mse: 2.9902\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 3s 281ms/step - loss: 3.0942 - mse: 3.0942\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 2s 183ms/step - loss: 2.2878 - mse: 2.2878\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 2s 267ms/step - loss: 2.1441 - mse: 2.1441\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 1.8863 - mse: 1.8863\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 2s 191ms/step - loss: 3.2967 - mse: 3.2967\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 2s 210ms/step - loss: 3.3565 - mse: 3.3565\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 2s 206ms/step - loss: 2.5461 - mse: 2.5461\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 2s 242ms/step - loss: 3.5510 - mse: 3.5510\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 2s 210ms/step - loss: 3.3732 - mse: 3.3732\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 2s 205ms/step - loss: 2.4054 - mse: 2.4054\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 2.3758 - mse: 2.3758\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 2s 180ms/step - loss: 2.8207 - mse: 2.8207\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 2s 181ms/step - loss: 2.1620 - mse: 2.1620\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 1s 162ms/step - loss: 2.4961 - mse: 2.4961\n",
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, None, 100)         44400     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, None, 100)         60400     \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, None, 1)           101       \n",
      "=================================================================\n",
      "Total params: 104,901\n",
      "Trainable params: 104,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb6a3606ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 64ms/step - loss: 0.7124 - mse: 0.7124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7123511433601379, 0.7123511433601379]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = bi_stack_lstm_model()\n",
    "model.fit(data_gen_train, epochs=100, callbacks=[early_stop])\n",
    "model.summary()\n",
    "model.evaluate(data_gen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-wedding",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 4s 89ms/step - loss: 22.4418 - mse: 22.4418\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 77ms/step - loss: 4.5435 - mse: 4.5435\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 80ms/step - loss: 3.2138 - mse: 3.2138\n",
      "Epoch 4/100\n",
      "3/9 [=========>....................] - ETA: 0s - loss: 1.8505 - mse: 1.8505"
     ]
    }
   ],
   "source": [
    "model = bi_back_lstm_model()\n",
    "model.fit(data_gen_train, epochs=100, callbacks=[early_stop])\n",
    "model.summary()\n",
    "model.evaluate(data_gen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-organ",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bi_back_stack_lstm_model()\n",
    "model.fit(data_gen_train, epochs=100, callbacks=[early_stop])\n",
    "model.summary()\n",
    "model.evaluate(data_gen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "trying-investigator",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 4s 59ms/step - loss: 21.3409 - mse: 21.3409\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 2.6011 - mse: 2.6011\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 3.7139 - mse: 3.7139: 0s - loss: 4.4127 - mse: 4.4\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 54ms/step - loss: 2.9117 - mse: 2.9117\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 3.3292 - mse: 3.3292\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 2.5813 - mse: 2.5813\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 2.9778 - mse: 2.9778\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 1.9670 - mse: 1.9670\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 2.8510 - mse: 2.8510\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 2.7390 - mse: 2.7390\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 2.4444 - mse: 2.4444\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 2.3526 - mse: 2.3526\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 0s 53ms/step - loss: 3.3229 - mse: 3.3229\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 0s 52ms/step - loss: 2.7993 - mse: 2.7993\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 2.5188 - mse: 2.5188\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 0s 49ms/step - loss: 3.3397 - mse: 3.3397\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 3.5700 - mse: 3.5700\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 0s 50ms/step - loss: 2.7748 - mse: 2.7748\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 0s 49ms/step - loss: 2.7494 - mse: 2.7494\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 3.5369 - mse: 3.5369\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 0s 52ms/step - loss: 2.2475 - mse: 2.2475\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 0s 53ms/step - loss: 2.5256 - mse: 2.5256\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 2.8006 - mse: 2.8006\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 2.2351 - mse: 2.2351\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 0s 47ms/step - loss: 2.7074 - mse: 2.7074\n",
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_53 (LSTM)               (None, None, 50)          22200     \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, None, 1)           51        \n",
      "=================================================================\n",
      "Total params: 22,251\n",
      "Trainable params: 22,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb6a432f1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 23ms/step - loss: 0.5917 - mse: 0.5917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5917391777038574, 0.5917391777038574]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = stack_3_lstm_model()\n",
    "model.fit(data_gen_train, epochs=100, callbacks=[early_stop])\n",
    "model.summary()\n",
    "model.evaluate(data_gen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "auburn-boating",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 3s 91ms/step - loss: 27.4856 - mse: 27.4856\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 14.2364 - mse: 14.2364\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 9.7113 - mse: 9.7113\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 10.8618 - mse: 10.8618\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 9.5097 - mse: 9.5097\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 9.3194 - mse: 9.3194\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 8.4292 - mse: 8.4292\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 90ms/step - loss: 8.9412 - mse: 8.9412\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 107ms/step - loss: 8.4190 - mse: 8.4190\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 8.9947 - mse: 8.9947\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 6.1738 - mse: 6.1738\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 7.9929 - mse: 7.9929\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 0s 53ms/step - loss: 6.3102 - mse: 6.3102\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 5.7442 - mse: 5.7442\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 5.7570 - mse: 5.7570\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 0s 45ms/step - loss: 6.1956 - mse: 6.1956\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 0s 53ms/step - loss: 5.9808 - mse: 5.9808\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 1s 51ms/step - loss: 4.9095 - mse: 4.9095\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 5.2063 - mse: 5.2063\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 1s 77ms/step - loss: 4.8520 - mse: 4.8520\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 1s 75ms/step - loss: 5.1667 - mse: 5.1667\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 4.7916 - mse: 4.7916\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 4.4450 - mse: 4.4450\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 4.8740 - mse: 4.8740\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 4.8210 - mse: 4.8210\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 1s 80ms/step - loss: 3.8388 - mse: 3.8388\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 1s 92ms/step - loss: 3.8440 - mse: 3.8440\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 3.6934 - mse: 3.6934\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 4.1962 - mse: 4.1962\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 3.6904 - mse: 3.6904\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 0s 50ms/step - loss: 4.1436 - mse: 4.1436\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 1s 53ms/step - loss: 4.3762 - mse: 4.3762\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 1s 92ms/step - loss: 4.8084 - mse: 4.8084\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 1s 95ms/step - loss: 3.3308 - mse: 3.3308\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 4.1803 - mse: 4.1803\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 4.1876 - mse: 4.1876\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 3.8865 - mse: 3.8865\n",
      "Epoch 38/100\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 3.3726 - mse: 3.3726\n",
      "Epoch 39/100\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 4.1427 - mse: 4.1427\n",
      "Epoch 40/100\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 4.7490 - mse: 4.7490\n",
      "Epoch 41/100\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 4.3542 - mse: 4.3542\n",
      "Epoch 42/100\n",
      "9/9 [==============================] - 1s 97ms/step - loss: 4.1289 - mse: 4.1289\n",
      "Epoch 43/100\n",
      "9/9 [==============================] - 1s 51ms/step - loss: 3.9881 - mse: 3.9881\n",
      "Epoch 44/100\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 3.7105 - mse: 3.7105\n",
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_54 (LSTM)               (None, 50)                22200     \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 32)                1632      \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "activation_29 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 25,977\n",
      "Trainable params: 25,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb6ac34fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 42ms/step - loss: 1.1201 - mse: 1.1201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.120126485824585, 1.120126485824585]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = deep_lstm_model()\n",
    "model.fit(data_gen_train, epochs=100, callbacks=[early_stop])\n",
    "model.summary()\n",
    "model.evaluate(data_gen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "neural-brazil",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 3s 76ms/step - loss: 12.8647 - mse: 12.8647\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 76ms/step - loss: 1.3657 - mse: 1.3657\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 55ms/step - loss: 3.3670 - mse: 3.3670\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 99ms/step - loss: 3.1556 - mse: 3.1556\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 3.6876 - mse: 3.6876\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 2.4851 - mse: 2.4851\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 2.3636 - mse: 2.3636\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 3.7620 - mse: 3.7620\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 2.6443 - mse: 2.6443\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 3.1290 - mse: 3.1290\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 99ms/step - loss: 3.1962 - mse: 3.1962\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 0s 50ms/step - loss: 2.6084 - mse: 2.6084\n",
      "Model: \"sequential_60\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_55 (LSTM)               (None, 50)                22200     \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 32)                1632      \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 23,865\n",
      "Trainable params: 23,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb69e43ad30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 19ms/step - loss: 1.6426 - mse: 1.6426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.642591118812561, 1.642591118812561]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lstm_dense_model()\n",
    "model.fit(data_gen_train, epochs=100, callbacks=[early_stop])\n",
    "model.summary()\n",
    "model.evaluate(data_gen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cooked-locking",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 4s 89ms/step - loss: 32.2926 - mse: 32.2926\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 16.4980 - mse: 16.4980\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 10.7462 - mse: 10.7462\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 7.6862 - mse: 7.6862\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 9.0544 - mse: 9.0544\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 12.3728 - mse: 12.3728\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 6.2003 - mse: 6.2003\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 3.1740 - mse: 3.1740\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 92ms/step - loss: 3.8392 - mse: 3.8392\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 3.5257 - mse: 3.5257\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 2.6273 - mse: 2.6273\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 2.9208 - mse: 2.9208\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 2.3529 - mse: 2.3529\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 2.5613 - mse: 2.5613\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 2.8537 - mse: 2.8537\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 2.6982 - mse: 2.6982\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 0s 54ms/step - loss: 1.9166 - mse: 1.9166\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 1s 55ms/step - loss: 3.3269 - mse: 3.3269\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 3.5767 - mse: 3.5767\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 2.5770 - mse: 2.5770\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 3.5259 - mse: 3.5259\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 1s 55ms/step - loss: 2.6127 - mse: 2.6127\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 1s 57ms/step - loss: 2.3053 - mse: 2.3053\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 2.6297 - mse: 2.6297\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 0s 53ms/step - loss: 3.3248 - mse: 3.3248\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 1s 52ms/step - loss: 2.1497 - mse: 2.1497\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 2.5460 - mse: 2.5460\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 2.7750 - mse: 2.7750\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 2.3429 - mse: 2.3429\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 1s 54ms/step - loss: 3.7385 - mse: 3.7385\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 0s 51ms/step - loss: 2.4512 - mse: 2.4512\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 2.2132 - mse: 2.2132\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 4.1836 - mse: 4.1836\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 3.0480 - mse: 3.0480\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 0s 51ms/step - loss: 2.3920 - mse: 2.3920\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 3.5548 - mse: 3.5548\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 1s 67ms/step - loss: 2.4336 - mse: 2.4336\n",
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_56 (LSTM)               (None, 50)                22200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 32)                1632      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 24,193\n",
      "Trainable params: 24,029\n",
      "Non-trainable params: 164\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb69f1b6ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 20ms/step - loss: 5.6540 - mse: 5.6540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.653958320617676, 5.653958320617676]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = batch_norm_lstm_model()\n",
    "model.fit(data_gen_train, epochs=100, callbacks=[early_stop])\n",
    "model.summary()\n",
    "model.evaluate(data_gen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "running-regular",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 3s 76ms/step - loss: 17.6656 - mse: 17.6656\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 91ms/step - loss: 3.3521 - mse: 3.3521\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 2.6646 - mse: 2.6646\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 3.2158 - mse: 3.2158\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 2.9120 - mse: 2.9120\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 1s 54ms/step - loss: 2.1421 - mse: 2.1421\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 2.1811 - mse: 2.1811\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 3.2929 - mse: 3.2929\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 2.7245 - mse: 2.7245\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 1.7782 - mse: 1.7782\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 1s 87ms/step - loss: 2.6393 - mse: 2.6393\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 2.7702 - mse: 2.7702\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 1s 95ms/step - loss: 2.8745 - mse: 2.8745\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 2.7923 - mse: 2.7923\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 76ms/step - loss: 2.2836 - mse: 2.2836\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 1s 79ms/step - loss: 2.3877 - mse: 2.3877\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 1s 76ms/step - loss: 2.2096 - mse: 2.2096\n",
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_57 (LSTM)               (None, 50)                22200     \n",
      "_________________________________________________________________\n",
      "layer_normalization_4 (Layer (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 32)                1632      \n",
      "_________________________________________________________________\n",
      "layer_normalization_5 (Layer (None, 32)                64        \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 24,029\n",
      "Trainable params: 24,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb69b32c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 26ms/step - loss: 0.7952 - mse: 0.7952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7952393293380737, 0.7952393889427185]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = layer_norm_lstm_model()\n",
    "model.fit(data_gen_train, epochs=100, callbacks=[early_stop])\n",
    "model.summary()\n",
    "model.evaluate(data_gen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "sapphire-footage",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 3s 58ms/step - loss: 37.4272 - mse: 37.4272\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 1s 53ms/step - loss: 19.7787 - mse: 19.7787\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 0s 53ms/step - loss: 12.8521 - mse: 12.8521\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 0s 51ms/step - loss: 9.2256 - mse: 9.2256\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 0s 51ms/step - loss: 8.3054 - mse: 8.3054\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 0s 49ms/step - loss: 9.1121 - mse: 9.1121\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 0s 53ms/step - loss: 8.0718 - mse: 8.0718\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 0s 51ms/step - loss: 7.7608 - mse: 7.7608\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 0s 52ms/step - loss: 6.9517 - mse: 6.9517\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 0s 52ms/step - loss: 6.5395 - mse: 6.5395\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 0s 51ms/step - loss: 6.7546 - mse: 6.7546\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 1s 50ms/step - loss: 6.1788 - mse: 6.1788\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 0s 51ms/step - loss: 6.2402 - mse: 6.2402\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 7.3335 - mse: 7.3335\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 1s 104ms/step - loss: 6.4010 - mse: 6.4010\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 6.5870 - mse: 6.5870\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 1s 90ms/step - loss: 5.1318 - mse: 5.1318\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 6.1761 - mse: 6.1761\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 5.9517 - mse: 5.9517\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 1s 69ms/step - loss: 6.0364 - mse: 6.0364\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 5.0534 - mse: 5.0534\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 5.2807 - mse: 5.2807\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 1s 88ms/step - loss: 5.2603 - mse: 5.2603\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 1s 61ms/step - loss: 5.2642 - mse: 5.2642\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 1s 59ms/step - loss: 6.0783 - mse: 6.0783\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 1s 54ms/step - loss: 5.0458 - mse: 5.0458\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 5.3426 - mse: 5.3426\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 5.3975 - mse: 5.3975\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 4.7080 - mse: 4.7080\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 4.6092 - mse: 4.6092\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 5.7744 - mse: 5.7744\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 6.1071 - mse: 6.1071\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 1s 91ms/step - loss: 5.3908 - mse: 5.3908\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 4.4363 - mse: 4.4363\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 4.5571 - mse: 4.5571\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 1s 56ms/step - loss: 5.1589 - mse: 5.1589\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 1s 80ms/step - loss: 4.8915 - mse: 4.8915\n",
      "Epoch 38/100\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 4.4990 - mse: 4.4990\n",
      "Epoch 39/100\n",
      "9/9 [==============================] - 1s 80ms/step - loss: 4.8152 - mse: 4.8152\n",
      "Epoch 40/100\n",
      "9/9 [==============================] - 1s 63ms/step - loss: 4.5721 - mse: 4.5721\n",
      "Epoch 41/100\n",
      "9/9 [==============================] - 1s 75ms/step - loss: 4.6164 - mse: 4.6164\n",
      "Epoch 42/100\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 4.6646 - mse: 4.6646\n",
      "Epoch 43/100\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 4.1865 - mse: 4.1865\n",
      "Epoch 44/100\n",
      "9/9 [==============================] - 1s 87ms/step - loss: 3.9607 - mse: 3.9607\n",
      "Epoch 45/100\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 4.8746 - mse: 4.8746\n",
      "Epoch 46/100\n",
      "9/9 [==============================] - 1s 66ms/step - loss: 5.1915 - mse: 5.1915\n",
      "Epoch 47/100\n",
      "9/9 [==============================] - 1s 65ms/step - loss: 4.9566 - mse: 4.9566\n",
      "Epoch 48/100\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 4.8354 - mse: 4.8354\n",
      "Epoch 49/100\n",
      "9/9 [==============================] - 1s 87ms/step - loss: 4.5227 - mse: 4.5227\n",
      "Epoch 50/100\n",
      "9/9 [==============================] - 1s 62ms/step - loss: 4.7562 - mse: 4.7562\n",
      "Epoch 51/100\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 4.2284 - mse: 4.2284\n",
      "Epoch 52/100\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 5.2143 - mse: 5.2143\n",
      "Epoch 53/100\n",
      "9/9 [==============================] - 1s 68ms/step - loss: 3.9824 - mse: 3.9824\n",
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_58 (LSTM)               (None, 50)                22200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 32)                1632      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 24,193\n",
      "Trainable params: 24,029\n",
      "Non-trainable params: 164\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb69875fa60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 31ms/step - loss: 3.8861 - mse: 3.8861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.886112689971924, 3.886112689971924]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## much better with drop out than without. \n",
    "model = batch_norm_drop_lstm_model()\n",
    "model.fit(data_gen_train, epochs=100, callbacks=[early_stop])\n",
    "model.summary()\n",
    "model.evaluate(data_gen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "neutral-manner",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 3s 58ms/step - loss: 5.1317 - mse: 5.1317\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 3.1296 - mse: 3.1296\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 0s 34ms/step - loss: 2.4490 - mse: 2.4490\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 3.5780 - mse: 3.5780\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 0s 39ms/step - loss: 2.1589 - mse: 2.1589\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 0s 54ms/step - loss: 1.9563 - mse: 1.9563\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 0s 33ms/step - loss: 2.2692 - mse: 2.2692\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 0s 50ms/step - loss: 2.6931 - mse: 2.6931\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 3.2158 - mse: 3.2158\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 0s 42ms/step - loss: 2.3302 - mse: 2.3302\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 1.9833 - mse: 1.9833\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 2.5412 - mse: 2.5412\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 1s 60ms/step - loss: 3.1143 - mse: 3.1143\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 0s 48ms/step - loss: 3.1521 - mse: 3.1521\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 0s 41ms/step - loss: 3.6887 - mse: 3.6887\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 0s 37ms/step - loss: 3.0034 - mse: 3.0034\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 0s 32ms/step - loss: 4.2499 - mse: 4.2499\n",
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_4 (GRU)                  (None, 50)                16800     \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 16,851\n",
      "Trainable params: 16,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb69884a5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.7834 - mse: 0.7834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7834493517875671, 0.7834493517875671]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gru_model()\n",
    "model.fit(data_gen_train, epochs=100, callbacks=[early_stop])\n",
    "model.summary()\n",
    "model.evaluate(data_gen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-campbell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-yukon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
